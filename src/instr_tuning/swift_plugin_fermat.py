import asyncio
import re
from typing import List

import json
from transformers import AutoTokenizer
from openai import OpenAI
from swift.plugin import ORM, orms
from swift.utils import get_logger

logger = get_logger()

PATTERN = r"^<think>.*?</think>\s*<correctness>(.*?)</correctness>(?![\s\S])"

PATTERN_DETAILED_AND_LOCALIZATION = r"^<think>(.*?)</think>\s*<correctness>(.*?)</correctness>\s*<localization>(.*?)</localization>(?![\s\S])"

QWQ_USER_PROMPT_REWARD = """\
I will show you a [Math Problem], a [Reference Solution], a [Student's Solution], and an [AI's Generated Error Location] generated by an AI assistant. Your task is to determine if the error location indicated in the [AI's Generated Error Location] correctly identify the error in the student's solution.

--------------------------------------------------

[Math Problem]

{problem}

[Reference Solution]

{reference}

[Student's Solution]

{student}

[AI's Generated Error Location]

{error_location}

--------------------------------------------------

Please evaluate whether the error location indicated in the [AI's Generated Error Location] is correct. The generated error location must be a specific mathematical expression in the [Student's Solution]. If [AI's Generated Error Location] only some generic error location like "Calculation", "Arithmetic", or "Conceptual Error", please answer "No".

Note that in order to be accepted as correct, the student's solution must provide intermediate calculations that are correct and lead to the final answer.

Reply with only "Yes" or "No".
"""

class StructureReward(ORM):
    def __call__(self, completions, **kwargs) -> List[float]:
        """
        Evaluates completions based on the correctness of the structure
        Args:
            completions (list[str]): Generated outputs
        Returns:
            list[float]: Reward scores
        """
    
        matches = [re.match(PATTERN, content, re.DOTALL | re.MULTILINE) for content in completions]
        return [1.0 if match else 0.0 for match in matches]


class CorrectnessAlignmentReward(ORM):
    def __call__(self, completions, **kwargs) -> List[float]:
        """
        Evaluates completions based on Mathematical correctness of the answer
        Args:
            completions (list[str]): Generated outputs
        Returns:
            list[float]: Reward scores
        """
        matches = [re.match(PATTERN, content, re.DOTALL | re.MULTILINE) for content in completions]
        correctness = [match.group(2).strip() if match else None for match in matches]

        # Check if the correctness is either "Correct" or "Incorrect"
        return [1.0 if c is not None and c.lower() in ["correct", "incorrect"] else 0.0 for c in correctness]
    
class CorrectnessMatchReward(ORM):
    def __call__(self, completions, labels, **kwargs) -> List[float]:
        """
        Evaluates completions based on Mathematical correctness of the answer
        Args:
            completions (list[str]): Generated outputs
            labels (list[str]): Expected answers
        Returns:
            list[float]: Reward scores
        """
        matches = [re.match(PATTERN_DETAILED_AND_LOCALIZATION, content, re.DOTALL | re.MULTILINE) for content in completions]
        correctness = [match.group(2).strip() if match else None for match in matches]
        localization = [match.group(3).strip() if match else None for match in matches]

        # Check if the correctness is either "Correct" or "Incorrect"
        # Also, if the correctness is Correct, the localization must be None
        rewards = []
        for c, l, label in zip(correctness, localization, labels):
            if c is None:
                rewards.append(0.0)
            elif c.lower() not in ["correct", "incorrect"]:
                rewards.append(0.0)
            elif c.lower() == "correct" and l != "None":
                rewards.append(0.0) 
            elif c.lower() == label.lower():
                rewards.append(1.0)
            else:
                rewards.append(0.0)
        return rewards
    


class CosineReward(ORM):
    # https://arxiv.org/abs/2502.03373
    def __init__(self,
                 tokenizer=None,
                 cosine_min_len_value_wrong: float = -0.5,
                 cosine_max_len_value_wrong: float = 0.0,
                 cosine_min_len_value_correct: float = 1.0,
                 cosine_max_len_value_correct: float = 0.5,
                 cosine_max_len: int = 1000,
                 accuracy_orm=None):
        self.tokenizer = tokenizer
        self.min_len_value_wrong = cosine_min_len_value_wrong
        self.max_len_value_wrong = cosine_max_len_value_wrong
        self.min_len_value_correct = cosine_min_len_value_correct
        self.max_len_value_correct = cosine_max_len_value_correct
        self.max_len = cosine_max_len
        self.accuracy_orm = accuracy_orm or CorrectnessMatchReward()

    @staticmethod
    def cosfn(t, T, min_value, max_value):
        import math
        return max_value - (max_value - min_value) * (1 - math.cos(t * math.pi / T)) / 2

    def __call__(self, completions, labels, **kwargs) -> List[float]:
        acc_rewards = self.accuracy_orm(completions, labels, **kwargs)
        rewards = []
        matches = [re.match(PATTERN_DETAILED_AND_LOCALIZATION, content, re.DOTALL | re.MULTILINE) for content in completions]
        thoughts = [match.group(1).strip() if match else None for match in matches]
        for content, acc_reward in zip(thoughts, acc_rewards):
            is_correct = acc_reward >= 1.
            if content is None:
                is_correct = False
                content = ""
            if is_correct:
                # Swap min/max for correct answers
                min_value = self.max_len_value_correct
                max_value = self.min_len_value_correct
            else:
                min_value = self.max_len_value_wrong
                max_value = self.min_len_value_wrong
            gen_len = len(self.tokenizer.encode(content))
            reward = self.cosfn(gen_len, self.max_len, min_value, max_value)
            rewards.append(reward)
        return rewards


class ThoughtLengthReward(ORM):
    def __init__(self, 
                 tokenizer=None,
                 min_len: int = 150, # From JudgeLRM
                 max_len: int = 1024):
        self.tokenizer = tokenizer
        self.min_len = min_len
        self.max_len = max_len
    def __call__(self, completions, **kwargs) -> List[float]:
        """
        Evaluates completions based on the length of the generated text
        Args:
            completions (list[str]): Generated outputs
        Returns:
            list[float]: Reward scores
        """
        rewards = []
        matches = [re.match(PATTERN_DETAILED_AND_LOCALIZATION, content, re.DOTALL | re.MULTILINE) for content in completions]
        thoughts = [match.group(1).strip() if match else None for match in matches]
        for thought in thoughts:
            if thought is None:
                rewards.append(0.0)
                continue
            gen_len = len(self.tokenizer.encode(thought))
            if gen_len < self.min_len:
                reward = 0.0
            elif gen_len > self.max_len:
                reward = -1.0
            else:
                reward = 0.25 # from JudgeLRM
            rewards.append(reward)
        return rewards

class RepetitionPenalty(ORM):
    # https://arxiv.org/abs/2502.03373
    def __init__(self, repetition_n_grams: int = 3, repetition_max_penalty: float = -0.5):
        self.ngram_size = repetition_n_grams
        self.max_penalty = repetition_max_penalty

    @staticmethod
    def zipngram(text: str, ngram_size: int):
        words = text.lower().split()
        return zip(*[words[i:] for i in range(ngram_size)])

    def __call__(self, completions, **kwargs) -> List[float]:
        """
        reward function the penalizes repetitions

        Args:
            completions: List of model completions
        """
        rewards = []
        matches = [re.match(PATTERN_DETAILED_AND_LOCALIZATION, content, re.DOTALL | re.MULTILINE) for content in completions]
        thoughts = [match.group(1).strip() if match else None for match in matches]
        for thought in thoughts:
            if thought is None:
                rewards.append(0.0)
                continue
            if thought == '':
                rewards.append(0.0)
                continue
            if len(thought.split()) < self.ngram_size:
                rewards.append(0.0)
                continue

            ngrams = set()
            total = 0
            for ng in self.zipngram(thought, self.ngram_size):
                ngrams.add(ng)
                total += 1

            scaling = 1 - len(ngrams) / total
            reward = scaling * self.max_penalty
            rewards.append(reward)
        return rewards
    

class LocalizationReward(ORM):

    def __init__(self):
        self.model = "Qwen/QwQ-32B-AWQ"
        self.tokenizer = AutoTokenizer.from_pretrained(self.model)
        self.client = OpenAI(
            base_url="http://__YOUR_IP_ADDRESS__:__YOUR_PORT__/v1",
            api_key="__YOUR_API_KEY__",
        )
        self.max_tokens = 1024

    def __call__(self, completions, labels, question, reference_answer, student_answer, **kwargs) -> List[float]:
        """
        Evaluates completions based on the correctness of the structure
        Args:
            completions (list[str]): Generated outputs
            labels (list[str]): Expected answers
        Returns:
            list[float]: Reward scores
        """
        matches = [re.match(PATTERN_DETAILED_AND_LOCALIZATION, content, re.DOTALL | re.MULTILINE) for content in completions]
        rewards = []
        questions = []
        references = []
        students = []
        error_locations = []
        for lab, match, q, ref, stu in zip(labels, matches, question, reference_answer, student_answer):
            if match is None:
                rewards.append(0.0)
                continue
            correctness = match.group(2).strip()
            error_localization = match.group(3).strip()
            if correctness.lower() != lab.lower():
                rewards.append(0.0)
                continue
            if lab.lower() == "correct":
                if error_localization != "None":
                    rewards.append(0.0)
                else:
                    rewards.append(1.0)
                continue
            if error_localization == "None":
                rewards.append(0.0)
                continue
            rewards.append(None)
            questions.append(q)
            references.append(ref)
            students.append(stu)
            error_locations.append(error_localization)
        
        if len(questions) == 0:
            return rewards
        
        r = self.localization_reward(
            questions, 
            references, 
            students, 
            error_locations,
            self.max_tokens,
        )
        for i, reward in enumerate(rewards):
            if reward is None:
                rewards[i] = r.pop(0)
        return rewards

    def localization_reward(self,
                            questions: list[str], 
                            references: list[str], 
                            students: list[str], 
                            error_locations: list[str], 
                            max_tokens: int=2048
                            ) -> list[float]:
    
        messages = [[
            {'role': 'user', 'content': QWQ_USER_PROMPT_REWARD.format(
                problem=question,
                reference=reference,
                student=student,
                error_location=error_location,
            )},
        ] for question, reference, student, error_location in zip(questions, references, students, error_locations)]
        prompts = self.tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        response = self.client.completions.create(
            model=self.model,
            prompt=prompts,
            temperature=0.8,
            max_tokens=max_tokens
        )

        rewards = []
        for i, res in enumerate(response.choices):
            num_tokens = (len(self.tokenizer.encode(res.text)))
            if num_tokens == max_tokens:
                new_prompt = prompts[i] + "\n\n".join(res.text.split("\n\n")[:-1]) + "\n</think>\n\n"
                response = self.client.completions.create(
                    model=self.model,
                    prompt=new_prompt,
                    temperature=0.8,
                    max_tokens=1,
                    logit_bias={
                        self.tokenizer.encode("Yes")[0]: 100,
                        self.tokenizer.encode("No")[0]: 100,
                    },
                )
                final_response = new_prompt + response.choices[0].text
            else:
                final_response = prompts[i] + res.text
            final_response = final_response.strip()
            if final_response.endswith("Yes"):
                rewards.append(1.0)
            elif final_response.endswith("No"):
                rewards.append(0.0)
            else:
                rewards.append(0.0)
        return rewards
    
        



orms['structure_reward'] = StructureReward
orms['correctness_alignment_reward'] = CorrectnessAlignmentReward
orms['correctness_match_reward'] = CorrectnessMatchReward
orms['cosine_reward'] = CosineReward
orms['thought_length_reward'] = ThoughtLengthReward
orms['repetition_penalty'] = RepetitionPenalty
orms['localization_reward'] = LocalizationReward
