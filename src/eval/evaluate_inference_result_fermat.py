
# python -m src.eval.evaluate_inference_result_fermat --model Qwen2.5-VL-7B-Instruct --eval_localization true


import re
# from src.prompts.qwen_base import PATTERN, SYSTEM_PROMPT, USER_PROMPT, USER_PROMPT_DETAILED_AND_LOCALIZATION
from openai import OpenAI
from transformers import AutoTokenizer
import base64
from io import BytesIO
import json
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
import argparse
from PIL import Image
import os
from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, average_precision_score
import pandas as pd

# PATTERN_DETAILED_AND_LOCALIZATION = r"<correctness>(.*?)</correctness>"
PATTERN_DETAILED_AND_LOCALIZATION = r"<correctness>(.*?)</correctness>\s*<localization>(.*?)</localization>"

ERROR_LOCALIZATION_REWARD_MODEL = "Qwen/QwQ-32B-AWQ"
tokenizer = AutoTokenizer.from_pretrained(ERROR_LOCALIZATION_REWARD_MODEL)

QWQ_USER_PROMPT_REWARD = """\
I will show you a [Math Problem], a [Reference Solution], a [Student's Solution], and an [AI's Generated Error Location] generated by an AI assistant. Your task is to determine if the error location indicated in the [AI's Generated Error Location] correctly identify the error in the student's solution.

--------------------------------------------------

[Math Problem]

{problem}

[Reference Solution]

{reference}

[Student's Solution]

{student}

[AI's Generated Error Location]

{error_location}

--------------------------------------------------

Please evaluate whether the error location indicated in the [AI's Generated Error Location] is correct. The generated error location must be a specific mathematical expression in the [Student's Solution]. If [AI's Generated Error Location] only some generic error location like "Calculation", "Arithmetic", or "Conceptual Error", please answer "No".

Note that in order to be accepted as correct, the student's solution must provide intermediate calculations that are correct and lead to the final answer.

Reply with only "Yes" or "No".
"""


with open("./src/data/processed_data_wo_korean.json", "r", encoding="utf-8-sig") as f:
    processed_data = json.load(f)

qa_trans = pd.read_csv("./src/data/serve_question/question_data/valid_data_QA_trans.csv")

qwq_client = OpenAI(
    base_url="http://__YOUR_IP_ADDRESS__:__YOUR_PORT__/v1",
    api_key="__YOUR_API_KEY__",
)

def load_dataset(dataset_path):
    """Load the dataset from the specified path of jsonl file."""
    with open(dataset_path, "r") as f:
        dataset = [json.loads(line) for line in f]
    return dataset

def extract_correctness(response: str):
    """Extract correctness from the response using regex."""
    matches = re.search(PATTERN_DETAILED_AND_LOCALIZATION, response, re.DOTALL | re.MULTILINE)
    correctness = matches.group(1).strip().lower() if matches else ""
    return correctness

def extract_localization(response: str):
    """Extract localization from the response using regex."""
    matches = re.search(PATTERN_DETAILED_AND_LOCALIZATION, response, re.DOTALL | re.MULTILINE)
    localization = matches.group(2).strip() if matches else ""
    return localization

def evaluate_correctness(infernece_result: list):
    """Evaluate the correctness of the inference result."""
    y_true = []
    y_pred = []
    for item in tqdm(infernece_result):
        label = item["label"]
        predict = item["predict"]
        y_true.append(0 if label == "Incorrect" else 1)

        correctness = extract_correctness(predict)
        if len(correctness) == 0:
            y_pred.append(2) 
            continue

        correctness = correctness.lower()
        y_pred.append(0 if correctness == "incorrect" else 1)

    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, average="macro")
    recall = recall_score(y_true, y_pred, average="macro")
    f1 = f1_score(y_true, y_pred, average="macro")

    return accuracy, precision, recall, f1

def localization_reward(questions: list[str], 
                        references: list[str], 
                        students: list[str], 
                        error_locations: list[str], 
                        max_tokens: int=1024
                        ) -> list[float]:
    
        messages = [[
            {'role': 'user', 'content': QWQ_USER_PROMPT_REWARD.format(
                problem=question,
                reference=reference,
                student=student,
                error_location=error_location,
            )},
        ] for question, reference, student, error_location in zip(questions, references, students, error_locations)]
        prompts = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)
        response = qwq_client.completions.create(
            model=ERROR_LOCALIZATION_REWARD_MODEL,
            prompt=prompts,
            temperature=0.8,
            max_tokens=max_tokens
        )

        rewards = []
        for i, res in enumerate(response.choices):
            num_tokens = (len(tokenizer.encode(res.text)))
            if num_tokens == max_tokens:
                new_prompt = prompts[i] + "\n\n".join(res.text.split("\n\n")[:-1]) + "\n</think>\n\n"
                response = qwq_client.completions.create(
                    model=ERROR_LOCALIZATION_REWARD_MODEL,
                    prompt=new_prompt,
                    temperature=0.8,
                    max_tokens=1,
                    logit_bias={
                        tokenizer.encode("Yes")[0]: 100,
                        tokenizer.encode("No")[0]: 100,
                    },
                )
                final_response = new_prompt + response.choices[0].text
            else:
                final_response = prompts[i] + res.text
            final_response = final_response.strip()
            if final_response.endswith("Yes"):
                rewards.append(1)
            elif final_response.endswith("No"):
                rewards.append(0)
            else:
                rewards.append(0)
        return rewards


def evaluate_localization(infernece_result: list):
    """Evaluate the localization in the inference result."""
    y_true = []
    y_pred = []
    batch_size = 64

    y_preds = []
    y_trues = []
    for idx, _ in enumerate(tqdm(infernece_result[::batch_size])):
        y_true = []
        y_pred = []
        _from, _to = idx*batch_size, (idx+1)*batch_size
        questions = []
        references = []
        students = []
        error_locations = []
        for item in infernece_result[_from:_to]:
            label = item["label"]
            predict = item["predict"]
            y_true.append(1 if label == "Incorrect" else 0)

            localization = extract_localization(predict)
            if len(localization) == 0:
                y_pred.append(0) 
                continue
            if localization.lower() == "none":
                y_pred.append(0)
                continue
            if y_true[-1] == 0 and localization.lower() != "none":
                y_pred.append(1)
                continue

            localization = localization.lower()
            y_pred.append(None)
            data = item["data"]
            questions.append(data["question"])
            references.append(data["reference_answer"])
            students.append(data["student_answer"])
            error_locations.append(localization)

        # Call the localization reward model
        rewards = localization_reward(
            questions=questions,
            references=references,
            students=students,
            error_locations=error_locations,
        )
        for i, y in enumerate(y_pred):
            if y is None:
                y_pred[i] = rewards.pop(0)
        y_preds.extend(y_pred)
        y_trues.extend(y_true)

    # Calculate accuracy, precision, recall, F1 score, and AUC-ROC
    accuracy = accuracy_score(y_trues, y_preds)
    precision = precision_score(y_trues, y_preds)
    recall = recall_score(y_trues, y_preds)
    f1 = f1_score(y_trues, y_preds)
    return accuracy, precision, recall, f1, y_preds, y_trues


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description="Inference script for multimodal model.")
    parser.add_argument("--base_url", type=str, default="http://localhost", help="base url for inference server")
    parser.add_argument("--port", type=int, default=8011, help="port number for inference server")
    parser.add_argument("--model", type=str, default="Qwen/Qwen2.5-VL-7B-Instruct", help="Model ID")
    parser.add_argument("--dataset", type=str, default="src/data/test_fermat_full_resolution.jsonl", help="Dataset path")
    parser.add_argument("--num_workers", type=int, default=8, help="Number of workers for requesting")
    parser.add_argument("--save_path", type=str, default="src/eval/results/fermat", help="Path to save the inference results")
    parser.add_argument("--eval_localization", type=bool, default=False, help="Whether to evaluate localization or not")

    args = parser.parse_args()

    qwq_client = OpenAI(
        base_url=f"{args.base_url}:{args.port}/v1",
        api_key="vehm",
    )

    print("Loading inference results...")

    folder_name = args.model.split("/")[-1]
    save_dir = os.path.join(args.save_path, folder_name)
    with open(os.path.join(save_dir, "inference_result.json"), "r") as f:
        inference_result = json.load(f)

    print("Inference results saved to", args.save_path)

    print("Starting evaluation...")
    print("Evaluating Error Detection Results...")
    detection_accuracy, detection_precision, detection_recall, detection_f1 = evaluate_correctness(inference_result)
    print(f"Accuracy: {detection_accuracy:.4f}")
    print(f"Precision: {detection_precision:.4f}")
    print(f"Recall: {detection_recall:.4f}")
    print(f"F1 Score: {detection_f1:.4f}")
    print("Finshed evaluating Error Detection Results.")

    if args.eval_localization:
        print("Evaluating Localization Results...")
        localize_accuracy, localize_precision, localize_recall, localize_f1, y_pred, y_true = evaluate_localization(inference_result)
        print(f"Accuracy: {localize_accuracy:.4f}")
        print(f"Precision: {localize_precision:.4f}")
        print(f"Recall: {localize_recall:.4f}")
        print(f"F1 Score: {localize_f1:.4f}")
        print("Finished evaluating Localization Results.")
        with open(os.path.join(save_dir, "metrics.csv"), "w") as f:
            f.write("Task,Accuracy,Precision,Recall,F1 Score\n")
            f.write(f"Error Detection,{detection_accuracy:.4f},{detection_precision:.4f},{detection_recall:.4f},{detection_f1:.4f}\n")
            f.write(f"Error Localization,{localize_accuracy:.4f},{localize_precision:.4f},{localize_recall:.4f},{localize_f1:.4f}\n")
        with open(os.path.join(save_dir, "error_localizaiton_results.csv"), "w") as f:
            f.write("True,Pred\n")
            for true, pred in zip(y_true, y_pred):
                f.write(f"{true},{pred}\n")
        print("Finished evaluating Error Localization Results.")
        print(f"Metrics saved to {os.path.join(save_dir, 'metrics.csv')}")
        print(f"Error localization results saved to {os.path.join(save_dir, 'error_localizaiton_results.csv')}")



    print("Evaluation completed.")